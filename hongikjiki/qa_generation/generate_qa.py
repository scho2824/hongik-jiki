from hongikjiki.tagging.tag_extractor import TagExtractor
from hongikjiki.tagging.tag_schema import TagSchema

import json
import logging
from typing import List, Dict
from tqdm import tqdm  # ì§„í–‰ ìƒí™© í‘œì‹œìš©

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Initialize tag extractor once for reuse
tag_schema = TagSchema("data/config/tag_schema.yaml")
tag_extractor = TagExtractor(tag_schema, "data/config/tag_patterns.json")

def load_dataset(input_path: str) -> List[Dict[str, str]]:
    with open(input_path, "r", encoding="utf-8") as f:
        return json.load(f)

def advanced_qa_generator(dataset: List[Dict[str, str]], min_length: int = 30) -> List[Dict[str, str]]:
    qa_pairs = []
    
    # ì§ˆë¬¸ í…œí”Œë¦¿
    question_templates = [
        "ì´ ë¬¸ì¥ì€ ë¬´ì—‡ì„ ë§í•˜ê³  ìˆë‚˜ìš”?",
        "ì´ ë‚´ìš©ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê°œë…ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì´ ê°€ë¥´ì¹¨ì˜ í•µì‹¬ ë©”ì‹œì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ì´ ë‚´ìš©ì„ ì¼ìƒì— ì–´ë–»ê²Œ ì ìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?",
        "ì´ ë‚´ìš©ì—ì„œ ê°•ì¡°í•˜ëŠ” ê°€ì¹˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    # ì²­í¬ ì²˜ë¦¬ í†µê³„
    total_chunks = len(dataset)
    too_short = 0
    processed = 0
    
    logger.info(f"ì´ {total_chunks}ê°œ ì²­í¬ ì²˜ë¦¬ ì‹œì‘")
    
    for item in tqdm(dataset):
        # Unify text field from content or page_content
        item["text"] = item.get("content") or item.get("page_content", "")
        text = item["text"]
        
        # ê¸¸ì´ í™•ì¸
        if len(text.strip()) < min_length:
            too_short += 1
            continue
        
        # íƒœê·¸ ì¶”ì¶œ
        try:
            tags = list(tag_extractor.extract_tags(text).keys())
        except Exception as e:
            logger.warning(f"Tag extraction failed for chunk: {e}")
            tags = []
        
        # ë‹¨ë½ ë‚´ìš©ì— ë”°ë¼ ì ì ˆí•œ ì§ˆë¬¸ ì„ íƒ
        # ê°„ë‹¨í•œ êµ¬í˜„: íƒœê·¸ì™€ í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¼ ì§ˆë¬¸ ì„ íƒ
        if len(tags) > 0 and 'ê¹¨ë‹¬ìŒ' in tags:
            question = question_templates[2]  # í•µì‹¬ ë©”ì‹œì§€
        elif len(tags) > 0 and 'ì‹¤ì²œ' in tags:
            question = question_templates[3]  # ì¼ìƒ ì ìš©
        elif len(text) > 500:
            question = question_templates[1]  # ì¤‘ìš” ê°œë…
        elif len(tags) > 0 and 'ê°€ì¹˜' in tags:
            question = question_templates[4]  # ê°•ì¡°í•˜ëŠ” ê°€ì¹˜
        else:
            question = question_templates[0]  # ê¸°ë³¸ ì§ˆë¬¸
        
        answer = text.strip()
        
        qa_pairs.append({
            "question": question, 
            "answer": answer, 
            "tags": tags
        })
        
        processed += 1
    
    # í†µê³„ ë¡œê¹…
    logger.info(f"ì²˜ë¦¬ ê²°ê³¼: ì´ {total_chunks}ê°œ ì¤‘ {processed}ê°œ ì²˜ë¦¬ë¨")
    logger.info(f"ë„ˆë¬´ ì§§ì•„ì„œ ê±´ë„ˆë›´ ì²­í¬: {too_short}ê°œ")
    logger.info(f"ìƒì„±ëœ QA ìŒ: {len(qa_pairs)}ê°œ")
    
    return qa_pairs

def save_qa_dataset(output_path: str, qa_pairs: List[Dict[str, str]]):
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", type=str, required=True)
    parser.add_argument("--output_file", type=str, required=True)
    parser.add_argument("--min_length", type=int, default=30, 
                       help="ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ (ì´ë³´ë‹¤ ì§§ì€ ì²­í¬ëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)")
    args = parser.parse_args()

    logger.info(f"ğŸ“¥ ì…ë ¥ íŒŒì¼: {args.input_file}")
    logger.info(f"ğŸ“¤ ì¶œë ¥ íŒŒì¼: {args.output_file}")

    dataset = load_dataset(args.input_file)
    qa_pairs = advanced_qa_generator(dataset, args.min_length)
    save_qa_dataset(args.output_file, qa_pairs)

    logger.info(f"âœ… ì´ {len(qa_pairs)}ê°œì˜ QA ìŒì´ ìƒì„±ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")