from hongikjiki.tagging.tag_extractor import TagExtractor
from hongikjiki.tagging.tag_schema import TagSchema

import json
import logging
from typing import List, Dict
from tqdm import tqdm  # ì§„í–‰ ìƒí™© í‘œì‹œìš©
import random

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Initialize tag extractor once for reuse
tag_schema = TagSchema("data/config/tag_schema.yaml")
tag_extractor = TagExtractor(tag_schema, "data/config/tag_patterns.json")

def generate_multiple_qa(text: str, tags: Dict[str, float]) -> List[Dict[str, str]]:
    qa_list = []

    # ë‹¤ì–‘í•œ ì§ˆë¬¸ í…œí”Œë¦¿ì—ì„œ ë¬´ì‘ìœ„ë¡œ 2ê°œ ì„ íƒ
    base_questions = [
        "ì´ ë¬¸ì¥ì—ì„œ ì „í•˜ë ¤ëŠ” í•µì‹¬ ê°œë…ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì´ ê°€ë¥´ì¹¨ì˜ ì¤‘ì‹¬ ë©”ì‹œì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ì´ ë‚´ìš©ì„ í†µí•´ ë¬´ì—‡ì„ ë°°ìš¸ ìˆ˜ ìˆë‚˜ìš”?",
        "ì´ ë‚´ìš©ì„ ì¼ìƒì— ì ìš©í•œë‹¤ë©´ ì–´ë–¤ ë³€í™”ê°€ ìˆì„ê¹Œìš”?",
        "ì´ ë‚´ìš©ì€ ì–´ë–¤ ì‚¶ì˜ íƒœë„ë¥¼ ê¶Œìœ í•˜ê³  ìˆë‚˜ìš”?",
        "ì´ ë‚´ìš©ì€ ë‹¹ì‹ ì˜ ê°€ì¹˜ê´€ì— ì–´ë–¤ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆë‚˜ìš”?",
    ]
    selected_questions = random.sample(base_questions, k=2)
    for q in selected_questions:
        qa_list.append({
            "question": q,
            "cleaned_question": q,
            "quoted_insight": text.strip().split(".")[0] + ".",
            "insight_explanation": "",
            "answer": text.strip(),
            "tags": list(tags.keys()),
            "source_text": text.strip()
        })

    # ì§ˆë¬¸ 3~4: íƒœê·¸ ê¸°ë°˜ ì§ˆë¬¸ (ìƒìœ„ íƒœê·¸ 1~2ê°œ)
    tag_insight_templates = [
        "ì´ ë‚´ìš©ì€ '{tag}'ì™€ ê´€ë ¨í•˜ì—¬ ì–´ë–¤ í†µì°°ì„ ì¤ë‹ˆê¹Œ?",
        "'{tag}'ë¼ëŠ” ê´€ì ì—ì„œ ì´ ë‚´ìš©ì„ ì–´ë–»ê²Œ í•´ì„í•  ìˆ˜ ìˆë‚˜ìš”?",
        "ì´ ë‚´ìš©ì€ '{tag}' ê°œë…ì„ ì–´ë–»ê²Œ ì„¤ëª…í•˜ê³  ìˆë‚˜ìš”?"
    ]

    # Safely obtain the schema dictionary regardless of attribute name
    internal_schema = getattr(tag_schema, "schema", getattr(tag_schema, "_schema", {}))
    tag_descriptions = {
        tag: internal_schema.get(tag, {}).get("description", tag)
        for tag in tags.keys()
    }

    top_tags = sorted(tags.items(), key=lambda x: x[1], reverse=True)[:2]
    for tag, _ in top_tags:
        description = tag_descriptions.get(tag, tag)
        question_template = random.choice(tag_insight_templates)
        qa_list.append({
            "question": question_template.format(tag=description),
            "cleaned_question": question_template.format(tag=description),
            "quoted_insight": text.strip().split(".")[0] + ".",
            "insight_explanation": "",
            "answer": text.strip(),
            "tags": [tag],
            "source_text": text.strip()
        })

    # ì¤‘ë³µ ì œê±° (ë³´ì¥ëœ íƒœê·¸ ìœ ì¼ì„±)
    for qa_item in qa_list:
        qa_item["tags"] = list(set(qa_item["tags"]))
    return qa_list

def load_dataset(input_path: str) -> List[Dict[str, str]]:
    with open(input_path, "r", encoding="utf-8") as f:
        return json.load(f)

def advanced_qa_generator(dataset: List[Dict[str, str]], min_length: int = 30) -> List[Dict[str, str]]:
    qa_pairs = []
    
    # ì§ˆë¬¸ í…œí”Œë¦¿
    question_templates = [
        "ì´ ë¬¸ì¥ì€ ë¬´ì—‡ì„ ë§í•˜ê³  ìˆë‚˜ìš”?",
        "ì´ ë‚´ìš©ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê°œë…ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì´ ê°€ë¥´ì¹¨ì˜ í•µì‹¬ ë©”ì‹œì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ì´ ë‚´ìš©ì„ ì¼ìƒì— ì–´ë–»ê²Œ ì ìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?",
        "ì´ ë‚´ìš©ì—ì„œ ê°•ì¡°í•˜ëŠ” ê°€ì¹˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    # ì²­í¬ ì²˜ë¦¬ í†µê³„
    total_chunks = len(dataset)
    too_short = 0
    processed = 0
    
    logger.info(f"ì´ {total_chunks}ê°œ ì²­í¬ ì²˜ë¦¬ ì‹œì‘")
    
    for item in tqdm(dataset):
        # Unify text field from content or page_content
        item["text"] = item.get("content") or item.get("page_content", "")
        text = item["text"]
        
        # ê¸¸ì´ í™•ì¸
        if len(text.strip()) < min_length:
            too_short += 1
            continue
        
        # íƒœê·¸ ì¶”ì¶œ
        try:
            main_tags, near_tags = tag_extractor.extract_tags(text, return_near=True)
            tags = main_tags if main_tags else dict(near_tags[:2])  # Use near tags if main_tags is empty
        except Exception as e:
            logger.warning(f"Tag extraction failed for chunk: {e}")
            tags = {}
        
        # ë‹¤ì–‘í•œ ì§ˆë¬¸ ìƒì„±: í•µì‹¬ ê°œë…, ì‹¤ì²œ, íƒœê·¸ ê¸°ë°˜ ë“±
        multiple_qa = generate_multiple_qa(text, tags)
        qa_pairs.extend(multiple_qa)
        
        processed += 1
    
    # í†µê³„ ë¡œê¹…
    logger.info(f"ì²˜ë¦¬ ê²°ê³¼: ì´ {total_chunks}ê°œ ì¤‘ {processed}ê°œ ì²˜ë¦¬ë¨")
    logger.info(f"ë„ˆë¬´ ì§§ì•„ì„œ ê±´ë„ˆë›´ ì²­í¬: {too_short}ê°œ")
    logger.info(f"ìƒì„±ëœ QA ìŒ: {len(qa_pairs)}ê°œ")
    
    return qa_pairs

def save_qa_dataset(output_path: str, qa_pairs: List[Dict[str, str]]):
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", type=str, required=True)
    parser.add_argument("--output_file", type=str, required=True)
    parser.add_argument("--min_length", type=int, default=30, 
                       help="ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ (ì´ë³´ë‹¤ ì§§ì€ ì²­í¬ëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)")
    args = parser.parse_args()

    logger.info(f"ğŸ“¥ ì…ë ¥ íŒŒì¼: {args.input_file}")
    logger.info(f"ğŸ“¤ ì¶œë ¥ íŒŒì¼: {args.output_file}")

    dataset = load_dataset(args.input_file)
    qa_pairs = advanced_qa_generator(dataset, args.min_length)
    save_qa_dataset(args.output_file, qa_pairs)

    logger.info(f"âœ… ì´ {len(qa_pairs)}ê°œì˜ QA ìŒì´ ìƒì„±ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")