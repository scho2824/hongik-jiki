"""
í™ìµì§€ê¸° ì±—ë´‡ í´ëž˜ìŠ¤ êµ¬í˜„
"""

import os
import sys
import logging
from typing import List, Dict, Any, Optional, Union
import traceback

from hongikjiki.text_processing.document_processor import DocumentProcessor
from hongikjiki.vector_store.chroma_store import ChromaVectorStore
from hongikjiki.vector_store.embeddings import get_embeddings
from hongikjiki.langchain_integration import (
    get_llm,
    ConversationMemory,
    ContextualMemory,
    ConversationalQAChain
)

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("HongikJikiChatBot")

class HongikJikiChatBot:
    """í™ìµì§€ê¸° ì±—ë´‡ í´ëž˜ìŠ¤"""
    
    def __init__(self, 
                 persist_directory: str = "./data",
                 embedding_type: str = "huggingface",
                 llm_type: str = "openai",
                 collection_name: str = "hongikjiki_documents",
                 max_history: int = 10,
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200,
                 use_tags: bool = True,
                 **kwargs):
        """
        í™ìµì§€ê¸° ì±—ë´‡ ì´ˆê¸°í™”
        
        Args:
            persist_directory: ë°ì´í„° ì €ìž¥ ë””ë ‰í† ë¦¬
            embedding_type: ìž„ë² ë”© ëª¨ë¸ íƒ€ìž…
            llm_type: ì–¸ì–´ ëª¨ë¸ íƒ€ìž…
            collection_name: ë²¡í„° ì €ìž¥ì†Œ ì»¬ë ‰ì…˜ ì´ë¦„
            max_history: ìµœëŒ€ ëŒ€í™” ê¸°ë¡ ìˆ˜
            chunk_size: ë¬¸ì„œ ì²­í¬ í¬ê¸°
            chunk_overlap: ë¬¸ì„œ ì²­í¬ ì¤‘ë³µ ì˜ì—­ í¬ê¸°
            use_tags: íƒœê·¸ ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
            **kwargs: ì¶”ê°€ ì„¤ì • ë§¤ê°œë³€ìˆ˜
        """
        self.persist_directory = persist_directory
        self.embedding_type = embedding_type
        self.llm_type = llm_type
        self.collection_name = collection_name
        self.max_history = max_history
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.use_tags = use_tags
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(persist_directory, exist_ok=True)
        
        # ìž„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embedding_kwargs = kwargs.get("embedding_kwargs", {})
        self.embeddings = get_embeddings(embedding_type, **embedding_kwargs)
        logger.info(f"ìž„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {embedding_type}")
        
        # ë²¡í„° ì €ìž¥ì†Œ ê²½ë¡œ ë¡œê¹…
        logger.info(f"ë²¡í„° ì €ìž¥ì†Œ ê²½ë¡œ: {os.path.abspath(persist_directory)}")
        
        # ë²¡í„° ì €ìž¥ì†Œ ì´ˆê¸°í™” - vector_store_dir ëŒ€ì‹  persist_directory ì‚¬ìš©
        self.vector_store = ChromaVectorStore(
            collection_name=collection_name,
            persist_directory=persist_directory,
            embeddings=self.embeddings
        )
        logger.info("ë²¡í„° ì €ìž¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.document_processor = DocumentProcessor(
            chunk_size=chunk_size,
            overlap=chunk_overlap
        )
        logger.info("ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
        # íƒœê·¸ ì¶”ì¶œê¸° ì´ˆê¸°í™” (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        self.tag_extractor = None
        if self.use_tags:
            try:
                # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
                project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
                if project_root not in sys.path:
                    sys.path.append(project_root)
                    
                from hongikjiki.tagging.tag_schema import TagSchema
                from hongikjiki.tagging.tag_extractor import TagExtractor
                
                # íƒœê·¸ ìŠ¤í‚¤ë§ˆ ë° íŒ¨í„´ íŒŒì¼ í™•ì¸
                tag_schema_path = os.path.join(project_root, 'data', 'config', 'tag_schema.yaml')
                tag_patterns_path = os.path.join(project_root, 'data', 'config', 'tag_patterns.json')
                
                if os.path.exists(tag_schema_path):
                    tag_schema = TagSchema(tag_schema_path)
                    self.tag_extractor = TagExtractor(
                        tag_schema, 
                        patterns_file=tag_patterns_path if os.path.exists(tag_patterns_path) else None
                    )
                    logger.info("íƒœê·¸ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except ImportError:
                logger.info("íƒœê·¸ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íƒœê·¸ ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
                self.use_tags = False
            except Exception as e:
                logger.warning(f"íƒœê·¸ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
                self.use_tags = False
        
        # LLM ì´ˆê¸°í™” (ì™¸ë¶€ ì£¼ìž… ì§€ì›)
        llm_kwargs = kwargs.get("llm_kwargs", {})
        if "llm" in kwargs and kwargs["llm"] is not None:
            self.llm = kwargs["llm"]
        else:
            self.llm = get_llm(llm_type, **llm_kwargs)
        logger.info(f"ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {llm_type}")
        
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        self.memory = ContextualMemory(max_history=max_history)
        logger.info("ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ëŒ€í™” ì²´ì¸ ì´ˆê¸°í™”
        self.chain = ConversationalQAChain(
            llm=self.llm,
            vector_store=self.vector_store,
            memory=self.memory,
            k=kwargs.get("search_results_count", 4)
        )
        logger.info("ëŒ€í™” ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        logger.info("í™ìµì§€ê¸° ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_documents(self, directory: str) -> int:
        """
        ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„° ì €ìž¥ì†Œì— ì¶”ê°€
        
        Args:
            directory: ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            int: ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜
        """
        try:
            logger.info(f"{directory} ë””ë ‰í† ë¦¬ì—ì„œ ë¬¸ì„œ ë¡œë“œ ì¤‘...")
            
            # ë¬¸ì„œ ì²˜ë¦¬
            documents = self.document_processor.process_directory(
                directory, 
                self.chunk_size, 
                self.chunk_overlap
            )
            
            if not documents:
                logger.warning("ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0
            
            # ë²¡í„° ì €ìž¥ì†Œì— ì¶”ê°€
            ids = self.vector_store.add_documents(documents)
            
            logger.info(f"ì´ {len(documents)}ê°œ ì²­í¬ ë²¡í„° ì €ìž¥ì†Œì— ì¶”ê°€ ì™„ë£Œ")
            return len(documents)
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ë¡œë“œ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            return 0
    
    def load_document(self, file_path: str) -> int:
        """
        ë‹¨ì¼ ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„° ì €ìž¥ì†Œì— ì¶”ê°€
        
        Args:
            file_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            int: ì²˜ë¦¬ëœ ì²­í¬ ìˆ˜
        """
        try:
            logger.info(f"{file_path} íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
            
            # ë¬¸ì„œ ì²˜ë¦¬
            chunks = self.document_processor.process_file(file_path)
            
            if not chunks:
                logger.warning("ì²˜ë¦¬ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0
            
            # ë²¡í„° ì €ìž¥ì†Œì— ì¶”ê°€
            ids = self.vector_store.add_documents(chunks)
            
            logger.info(f"ì´ {len(chunks)}ê°œ ì²­í¬ ë²¡í„° ì €ìž¥ì†Œì— ì¶”ê°€ ì™„ë£Œ")
            return len(chunks)
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            return 0
    
    def chat(self, user_input: str) -> str:
        """
        ì‚¬ìš©ìž ìž…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±

        Args:
            user_input: ì‚¬ìš©ìž ìž…ë ¥ ì§ˆë¬¸

        Returns:
            str: ìƒì„±ëœ ì‘ë‹µ
        """
        try:
            logger.info(f"ì‚¬ìš©ìž ì§ˆë¬¸: {user_input}")
            
            # íƒœê·¸ ì¶”ì¶œ (íƒœê·¸ ê¸°ëŠ¥ì´ í™œì„±í™”ëœ ê²½ìš°)
            extracted_tags = []
            if self.use_tags and self.tag_extractor:
                extracted_tags = self.tag_extractor.extract_tags_from_query(user_input)
                logger.info(f"ì¶”ì¶œëœ íƒœê·¸: {extracted_tags}")
            
            # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            logger.info("ë²¡í„° ê²€ìƒ‰ ì‹œìž‘...")
            # ê²€ìƒ‰ ìž„ê³„ê°’ ì„¤ì • - ì´ ê°’ë³´ë‹¤ ë‚®ì€ ìœ ì‚¬ë„ëŠ” ë¬´ì‹œ
            similarity_threshold = 0.3  
            
            # íƒœê·¸ ê¸°ë°˜ ê²€ìƒ‰ (íƒœê·¸ê°€ ìžˆëŠ” ê²½ìš°)
            if self.use_tags and extracted_tags and hasattr(self.vector_store, 'search_with_tags'):
                search_results = self.vector_store.search_with_tags(
                    user_input, 
                    tags=extracted_tags,
                    tag_boost=0.3,
                    k=7
                )
            else:
                search_results = self.vector_store.search(user_input, k=7)

            # ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…
            logger.info(f"ê²€ìƒ‰ ê²°ê³¼ {len(search_results)}ê°œ ì°¾ìŒ")
            
            # ìœ ì‚¬ë„ ì ìˆ˜ì— ë”°ë¼ í•„í„°ë§
            filtered_results = []
            for i, result in enumerate(search_results):
                score = result.get("score", 0)
                content = result.get("content", "")[:100]
                logger.info(f"ê²°ê³¼ {i+1}: ì ìˆ˜={score:.4f}, ë‚´ìš©={content}...")
                
                # ìž„ê³„ê°’ë³´ë‹¤ ë†’ì€ ê²°ê³¼ë§Œ ì‚¬ìš©
                if score >= similarity_threshold:
                    filtered_results.append(result)
            
            logger.info(f"ìž„ê³„ê°’({similarity_threshold}) ì´ìƒ ê²°ê³¼: {len(filtered_results)}ê°œ")
            
            # í•„í„°ë§ëœ ê²°ê³¼ë¥¼ ì‚¬ìš©
            search_results = filtered_results

            # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            if not search_results:
                logger.warning("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ê´€ë ¨ëœ ì •ë²• ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”."

            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ ë‚´ìš© ì¶”ì¶œ (ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©í•˜ì—¬ í† í° ì œí•œ)
            context_texts = []
            for i, result in enumerate(search_results[:3]):  # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
                content = result.get("content", "")
                
                # ì½˜í…ì¸  ê¸¸ì´ ì œí•œ (4000ìžë¡œ ì œí•œ)
                if len(content) > 4000:
                    content = content[:4000] + "..."
                
                score = result.get("score", 0)

                # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
                metadata = result.get("metadata", {})
                source_info = f"ì¶œì²˜: {metadata.get('title', 'ì œëª© ì—†ìŒ')}"
                if "lecture_number" in metadata:
                    source_info += f", ì •ë²• {metadata.get('lecture_number')}ê°•"
                
                # íƒœê·¸ ì •ë³´ ì¶”ê°€ (íƒœê·¸ê°€ ìžˆëŠ” ê²½ìš°)
                if "tags" in metadata:
                    tags = metadata.get("tags", [])
                    if tags and isinstance(tags, str):
                        tags = [tag.strip() for tag in tags.split(",")]
                    if tags:
                        source_info += f", íƒœê·¸: {', '.join(tags)}"
                
                # matching_tags í•„ë“œê°€ ìžˆëŠ” ê²½ìš° (ê²€ìƒ‰ ê²°ê³¼ ìž¬ìˆœìœ„í™” ì‹œ)
                if "matching_tags" in result:
                    matching_tags = result.get("matching_tags", [])
                    if matching_tags:
                        source_info += f", ì¼ì¹˜ íƒœê·¸: {', '.join(matching_tags)}"

                context_texts.append(f"{content}\n{source_info}")

            logger.info(f"ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ {len(context_texts)}ê°œ ìƒì„± (í† í° ì œí•œìœ¼ë¡œ ìµœëŒ€ 3ê°œë§Œ ì‚¬ìš©)")

            # ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (í† í° ì œí•œ í•´ê²°)
            prompt_template = """
ë‹¹ì‹ ì€ ì²œê³µ ìŠ¤ìŠ¹ë‹˜ì˜ ì •ë²• ê°€ë¥´ì¹¨ì— ê¸°ë°˜í•œ 'í™ìµì§€ê¸°' ì±—ë´‡ìž…ë‹ˆë‹¤. 

ì‚¬ìš©ìž ì§ˆë¬¸: {question}

ì°¸ê³ í•  ì •ë²• ë¬¸ì„œ:
{context}

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ì •ë²•ì˜ ê°€ë¥´ì¹¨ì„ ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ë©´ì„œ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
"""

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            context_text = "\n\n".join(context_texts)
            prompt = prompt_template.format(
                question=user_input,
                context=context_text
            )

            # ë¡œê¹… (ë””ë²„ê¹…ìš©)
            logger.info("LLM ì‘ë‹µ ìƒì„± ì‹œìž‘...")
            logger.debug(f"ìƒì„± í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ìž")

            # LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
            response = self.generate_text(prompt)
            logger.info("LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ")
            logger.debug(f"ìƒì„±ëœ ì‘ë‹µ: {response}")
            
            # ê´€ë ¨ ê°•ì˜ ì¶”ì²œ ì¶”ê°€ (íƒœê·¸ ê¸°ë°˜)
            related_lectures = self._extract_related_lectures(search_results)
            if related_lectures:
                response += "\n\nðŸ§­ *ë” ì•Œê³  ì‹¶ë‹¤ë©´, ë‹¤ìŒ ê°•ì˜ë¥¼ ì°¸ê³ í•´ë³´ì„¸ìš”:*\n"
                for lecture in related_lectures:
                    response += f"* {lecture}\n"

            return response

        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    
    def _extract_related_lectures(self, search_results: List[Dict[str, Any]]) -> List[str]:
        """
        ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ ê°•ì˜ ì •ë³´ ì¶”ì¶œ
        
        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[str]: ê´€ë ¨ ê°•ì˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        lectures = []
        seen_files = set()  # ì¤‘ë³µ ë°©ì§€ìš©
        
        for result in search_results:
            metadata = result.get("metadata", {})
            filename = metadata.get("filename", "")
            
            if filename in seen_files:
                continue
                
            # ê°•ì˜ ë²ˆí˜¸ê°€ ìžˆëŠ” ê²½ìš° ì‚¬ìš©
            lecture_num = metadata.get("lecture_number")
            title = metadata.get("title", "")
            
            if lecture_num:
                lecture = f"ì •ë²• {lecture_num}ê°•: {title}"
            else:
                # ê°•ì˜ ë²ˆí˜¸ê°€ ì—†ëŠ” ê²½ìš° íŒŒì¼ëª… ì‚¬ìš©
                lecture = filename.replace(".txt", "").replace("_", " ")
                
            lectures.append(lecture)
            seen_files.add(filename)
        
        return lectures[:3]  # ìµœëŒ€ 3ê°œë§Œ ë°˜í™˜
    
    def chat_with_context(self, message: str, context: Optional[str] = None) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì‚¬ìš©ìž ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            message: ì‚¬ìš©ìž ë©”ì‹œì§€
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒ ì‚¬í•­)
            
        Returns:
            str: ì±—ë´‡ ì‘ë‹µ
        """
        try:
            input_data = {
                "question": message,
                "context": context
            }
            
            # ì²´ì¸ì„ í†µí•´ ì‘ë‹µ ìƒì„±
            response = self.chain.run(input_data)
            
            return response
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def generate_text(self, prompt: str) -> str:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            prompt: í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸

        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        # llm ê°ì²´ê°€ self.llmì— ì €ìž¥ë˜ì–´ ìžˆë‹¤ê³  ê°€ì •
        return self.llm.generate_text(prompt)
    
    def add_context(self, key: str, value: Any) -> None:
        """
        ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ ì¶”ê°€
        
        Args:
            key: ì»¨í…ìŠ¤íŠ¸ í‚¤
            value: ì»¨í…ìŠ¤íŠ¸ ê°’
        """
        if isinstance(self.memory, ContextualMemory):
            self.memory.add_context(key, value)
            logger.debug(f"ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€: {key}")
    
    def clear_history(self) -> None:
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.memory.clear()
        logger.info("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def search_documents(self, query: str, k: int = 4) -> List[Dict[str, Any]]:
        """
        ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict]: ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # íƒœê·¸ ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if self.use_tags and hasattr(self.vector_store, 'advanced_search'):
                return self.vector_store.advanced_search(query, use_tags=True, k=k)
            else:
                return self.vector_store.search(query, k=k)
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_document_count(self) -> int:
        """
        ë²¡í„° ì €ìž¥ì†Œì˜ ë¬¸ì„œ ìˆ˜ ë°˜í™˜
        
        Returns:
            int: ë¬¸ì„œ ìˆ˜
        """
        try:
            return self.vector_store.count()
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê°œìˆ˜ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return 0
    
    def reset_vector_store(self) -> bool:
        """
        ë²¡í„° ì €ìž¥ì†Œ ì´ˆê¸°í™”
        
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            self.vector_store.reset()
            logger.info("ë²¡í„° ì €ìž¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"ë²¡í„° ì €ìž¥ì†Œ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return False
            
    def get_related_questions(self, user_question: str, max_questions: int = 3) -> List[str]:
        """
        íƒœê·¸ ê¸°ë°˜ ê´€ë ¨ ì§ˆë¬¸ ì¶”ì²œ
        
        Args:
            user_question: ì‚¬ìš©ìž ì§ˆë¬¸
            max_questions: ìµœëŒ€ ì¶”ì²œ ì§ˆë¬¸ ìˆ˜
            
        Returns:
            List[str]: ì¶”ì²œ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        if not self.use_tags or not self.tag_extractor:
            return []
        
        # íƒœê·¸ ì¶”ì¶œ
        query_tags = self.tag_extractor.extract_tags_from_query(user_question)
        if not query_tags:
            return []
        
        # íƒœê·¸ë³„ í…œí”Œë¦¿ ì§ˆë¬¸
        tag_questions = {
            "ì •ë²•": [
                "ì •ë²•ì´ëž€ ë¬´ì—‡ì¸ê°€ìš”?", 
                "ì •ë²•ì„ ì–´ë–»ê²Œ ì‹¤ì²œí•  ìˆ˜ ìžˆë‚˜ìš”?",
                "ì •ë²•ê³¼ ì¢…êµì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            ],
            "í™ìµì¸ê°„": [
                "í™ìµì¸ê°„ì´ëž€ ë¬´ì—‡ì¸ê°€ìš”?", 
                "í™ìµì¸ê°„ì„ ì‹¤ì²œí•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "í™ìµì¸ê°„ì˜ ì •ì‹ ìœ¼ë¡œ ì„¸ìƒì„ ë°”ë¼ë³´ë©´ ì–´ë–¤ ì ì´ ë‹¬ë¼ì§€ë‚˜ìš”?"
            ],
            "ìˆ˜í–‰": [
                "ì •ë²•ì—ì„œ ë§í•˜ëŠ” ìˆ˜í–‰ì´ëž€ ë¬´ì—‡ì¸ê°€ìš”?", 
                "ì¼ìƒì—ì„œ ìˆ˜í–‰ì„ ì–´ë–»ê²Œ í•  ìˆ˜ ìžˆë‚˜ìš”?",
                "ìˆ˜í–‰ì„ í†µí•´ ì–»ì„ ìˆ˜ ìžˆëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?"
            ],
            "ì¸ê°„ê´€ê³„": [
                "ì •ë²•ì—ì„œ ë§í•˜ëŠ” ì˜¬ë°”ë¥¸ ì¸ê°„ê´€ê³„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?", 
                "ì¸ê°„ê´€ê³„ì—ì„œ ê°ˆë“±ì´ ìƒê¸¸ ë•Œ ì–´ë–»ê²Œ í•´ê²°í•´ì•¼ í•˜ë‚˜ìš”?",
                "ê°€ì¡± ê´€ê³„ì—ì„œ ì–´ë ¤ì›€ì´ ìžˆì„ ë•Œ ì–´ë–»ê²Œ ëŒ€ì²˜í•´ì•¼ í•˜ë‚˜ìš”?"
            ],
            "ìžìœ ì˜ì§€": [
                "ì •ë²•ì—ì„œ ë§í•˜ëŠ” ìžìœ ì˜ì§€ëž€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ì„ íƒê³¼ ì±…ìž„ì˜ ê´€ê³„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "ì¸ê°„ì˜ ìžìœ ì˜ì§€ì™€ ìš°ì£¼ ë²•ì¹™ì€ ì–´ë–¤ ê´€ê³„ê°€ ìžˆë‚˜ìš”?"
            ],
            "ê¹¨ë‹¬ìŒ": [
                "ì •ë²•ì—ì„œ ë§í•˜ëŠ” ê¹¨ë‹¬ìŒì´ëž€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ê¹¨ë‹¬ìŒì— ì´ë¥´ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ê¹¨ë‹¬ì€ í›„ì˜ ì‚¶ì€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ë‚˜ìš”?"
            ],
            "ë¶ˆì•ˆ": [
                "ë¶ˆì•ˆí•œ ë§ˆìŒì„ ë‹¤ìŠ¤ë¦¬ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ê±±ì •ê³¼ ë¶ˆì•ˆì—ì„œ ë²—ì–´ë‚˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
                "ë¶ˆì•ˆì€ ì™œ ìƒê¸°ëŠ” ê±´ê°€ìš”?"
            ],
            "ë¶„ë…¸": [
                "ë¶„ë…¸ë¥¼ ë‹¤ìŠ¤ë¦¬ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "í™”ê°€ ë‚  ë•Œ ì–´ë–»ê²Œ ëŒ€ì²˜í•´ì•¼ í•˜ë‚˜ìš”?",
                "ë¶„ë…¸ì˜ ì›ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            ]
        }
        
        # ì¶”ì¶œëœ íƒœê·¸ì— í•´ë‹¹í•˜ëŠ” ì§ˆë¬¸ ìˆ˜ì§‘
        related_questions = []
        for tag in query_tags:
            if tag in tag_questions:
                related_questions.extend(tag_questions[tag])
        
        # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        unique_questions = []
        for q in related_questions:
            if q not in unique_questions:
                unique_questions.append(q)
                if len(unique_questions) >= max_questions:
                    break
        
        return unique_questions


    def get_related_tags(self, query: str, max_tags: int = 5) -> list:
        """
        ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ íƒœê·¸ ì¶”ì¶œ
        
        Args:
            query: ì‚¬ìš©ìž ì§ˆë¬¸
            max_tags: ìµœëŒ€ íƒœê·¸ ìˆ˜
            
        Returns:
            List[Dict]: íƒœê·¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        if not self.use_tags or not self.tag_extractor:
            return []
        
        # ì¿¼ë¦¬ì—ì„œ íƒœê·¸ ì¶”ì¶œ
        extracted_tags = self.tag_extractor.extract_tags_from_query(query)
        
        # íƒœê·¸ ìŠ¤í‚¤ë§ˆì—ì„œ íƒœê·¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        tag_info = []
        for tag in extracted_tags[:max_tags]:
            tag_obj = self.tag_extractor.tag_schema.get_tag(tag)
            if tag_obj:
                tag_info.append({
                    "name": tag,
                    "category": tag_obj.category,
                    "emoji": tag_obj.emoji or "",
                    "description": tag_obj.description or ""
                })
        
        return tag_info

    def search_documents_by_tag(self, tag: str, k: int = 5) -> list:
        """
        íŠ¹ì • íƒœê·¸ë¡œ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            tag: ê²€ìƒ‰í•  íƒœê·¸
            k: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict]: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.use_tags:
            return []
        
        # íƒœê·¸ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰
        try:
            if hasattr(self.vector_store, 'search_with_tags'):
                results = self.vector_store.search_with_tags("", [tag], tag_boost=1.0, k=k)
            else:
                # íƒœê·¸ ê¸°ë°˜ ê²€ìƒ‰ì´ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° íƒœê·¸ë¥¼ ì¿¼ë¦¬ë¡œ ì‚¬ìš©
                results = self.vector_store.search(f"tag:{tag}", k=k)
            
            # ê²°ê³¼ ê°€ê³µ
            search_results = []
            for result in results:
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œëª© ë° ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
                metadata = result.get("metadata", {})
                title = metadata.get("title", "ì œëª© ì—†ìŒ")
                source = metadata.get("source", "")
                lecture = metadata.get("lecture_number", "")
                
                # ì²­í¬ ì¼ë¶€ë§Œ ì‚¬ìš© (ë„ˆë¬´ ê¸¸ë©´ UIì— í‘œì‹œí•˜ê¸° ì–´ë ¤ì›€)
                content = result.get("content", "")
                if len(content) > 200:
                    content = content[:200] + "..."
                
                search_results.append({
                    "title": title,
                    "content": content,
                    "source": f"{source} {lecture}ê°•" if lecture else source,
                    "score": result.get("score", 0)
                })
            
            return search_results
            
        except Exception as e:
            logger.error(f"íƒœê·¸ ê¸°ë°˜ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []