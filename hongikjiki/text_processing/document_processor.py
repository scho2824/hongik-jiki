import os
import logging
from typing import List, Dict, Any, Optional
import hashlib
import json
from datetime import datetime

from hongikjiki.text_processing.document_loader import DocumentLoader
from hongikjiki.text_processing.text_normalizer import TextNormalizer
from hongikjiki.text_processing.metadata_extractor import MetadataExtractor
from hongikjiki.text_processing.document_chunker import DocumentChunker

logger = logging.getLogger("HongikJikiChatBot")

def update_processed_log(file_path, content_hash, chunk_count, log_path="data/processed/processed_files.json"):
    try:
        with open(log_path, "r", encoding="utf-8") as f:
            record = json.load(f)
    except FileNotFoundError:
        record = {}

    record[file_path] = {
        "hash": content_hash,
        "processed_time": datetime.now().isoformat(),
        "chunks_count": chunk_count,
        "vector_ids": []
    }

    with open(log_path, "w", encoding="utf-8") as f:
        json.dump(record, f, ensure_ascii=False, indent=2)

class DocumentProcessor:
    """
    ë¬¸ì„œ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì¡°í•©í•˜ì—¬ ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ ê³¼ì •ì„ ì¡°ìœ¨í•¨
    """
    
    def __init__(self, 
                 document_loader: Optional[DocumentLoader] = None,
                 text_normalizer: Optional[TextNormalizer] = None,
                 metadata_extractor: Optional[MetadataExtractor] = None,
                 document_chunker: Optional[DocumentChunker] = None,
                 chunk_size: int = 1000,
                 overlap: int = 200):
        """
        DocumentProcessor ì´ˆê¸°í™” ë° ì˜ì¡´ì„± ì£¼ì…
        
        Args:
            document_loader: ë¬¸ì„œ ë¡œë” ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ìƒì„±)
            text_normalizer: í…ìŠ¤íŠ¸ ì •ê·œí™” ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ìƒì„±)
            metadata_extractor: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ìƒì„±)
            document_chunker: ë¬¸ì„œ ì²­í‚¹ ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ìƒì„±)
            chunk_size: ê¸°ë³¸ ì²­í¬ í¬ê¸°
            overlap: ê¸°ë³¸ ì¤‘ë³µ ì˜ì—­ í¬ê¸°
        """
        # ì˜ì¡´ì„± ì£¼ì… ë˜ëŠ” ìƒì„±
        self.document_loader = document_loader or DocumentLoader()
        self.text_normalizer = text_normalizer or TextNormalizer()
        self.metadata_extractor = metadata_extractor or MetadataExtractor()
        self.document_chunker = document_chunker or DocumentChunker(chunk_size, overlap)
        
        # ê¸°ë³¸ ì„¤ì •ê°’
        self.chunk_size = chunk_size
        self.overlap = overlap
        
        # ì²˜ë¦¬ëœ ë¬¸ì„œ í•´ì‹œ ì¶”ì  (ì¤‘ë³µ ê°ì§€ìš©)
        self.processed_hashes = []
    
    def process_directory(self, directory: str, chunk_size: int = None, overlap: int = None) -> List[Dict[str, Any]]:
        """
        ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬
        
        Args:
            directory: ì²˜ë¦¬í•  ë¬¸ì„œê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
            chunk_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’ ì‚¬ìš© ì‹œ None)
            overlap: ì¤‘ë³µ ì˜ì—­ í¬ê¸° (ê¸°ë³¸ê°’ ì‚¬ìš© ì‹œ None)
            
        Returns:
            List[Dict]: ì²˜ë¦¬ ë° ì²­í‚¹ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        # ê¸°ë³¸ê°’ ì„¤ì •
        chunk_size = chunk_size or self.chunk_size
        overlap = overlap or self.overlap
        
        logger.info(f"{directory} í´ë”ì—ì„œ ì •ë²• ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘...")
        
        documents = []
        processed_files = 0
        skipped_files = 0
        
        for filename in os.listdir(directory):
            file_path = os.path.join(directory, filename)

            # ì²˜ë¦¬ ë¡œê·¸ ë¡œë“œ
            log_path = "data/processed/processed_files.json"
            try:
                with open(log_path, "r", encoding="utf-8") as f:
                    processed_log = json.load(f)
            except FileNotFoundError:
                processed_log = {}

            if file_path in processed_log:
                logger.info(f"ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼: {filename} â†’ ê±´ë„ˆëœ€")
                skipped_files += 1
                continue

            # ë””ë ‰í† ë¦¬ ê±´ë„ˆë›°ê¸°
            if os.path.isdir(file_path):
                logger.debug(f"ë””ë ‰í† ë¦¬ ê±´ë„ˆë›°ê¸°: {filename}")
                continue
            
            try:
                # 1. ë¬¸ì„œ ë¡œë“œ
                doc_data = self.document_loader.load_document(file_path)
                if not doc_data:
                    logger.warning(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {filename}")
                    skipped_files += 1
                    continue
                
                # 2. í…ìŠ¤íŠ¸ ì •ê·œí™”
                normalized_content = self.text_normalizer.normalize(doc_data["content"])
                
                # 3. ì¤‘ë³µ í™•ì¸
                content_hash = hashlib.md5(normalized_content.encode('utf-8')).hexdigest()
                if content_hash in self.processed_hashes:
                    logger.info(f"ì¤‘ë³µ ë¬¸ì„œ ê±´ë„ˆë›°ê¸°: {filename}")
                    skipped_files += 1
                    continue
                
                self.processed_hashes.append(content_hash)
                
                # 4. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata = self.metadata_extractor.extract_metadata(
                    normalized_content, 
                    filename, 
                    doc_data["metadata"]
                )
                
                # í•´ì‹œ ì—…ë°ì´íŠ¸
                metadata["file_hash"] = content_hash
                
                # ì²˜ë¦¬ëœ ë¬¸ì„œ ì¶”ê°€
                documents.append({
                    "content": normalized_content,
                    "metadata": metadata
                })
                
                processed_files += 1
                logger.debug(f"ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {filename}")
                
            except Exception as e:
                logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜: {filename}, ì˜¤ë¥˜: {e}")
                logger.exception(e)
                skipped_files += 1
        
        logger.info(f"ì´ {processed_files}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ, {skipped_files}ê°œ ë¬¸ì„œ ê±´ë„ˆëœ€")
        
        # 5. ë¬¸ì„œ ë¶„í•  (ì²­í‚¹)
        chunked_docs = self.document_chunker.split_documents(documents, chunk_size, overlap)
        logger.info(f"ì´ {len(chunked_docs)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        
        # ğŸ’¾ ì²˜ë¦¬ ê¸°ë¡ ì €ì¥ (ë¬¸ì„œë³„)
        for doc in documents:
            file_path = doc["metadata"]["source"]
            file_hash = doc["metadata"]["file_hash"]
            chunk_count = sum(1 for c in chunked_docs if c["metadata"]["source"] == file_path)
            update_processed_log(file_path, file_hash, chunk_count)
        
        return chunked_docs
    
    def process_file(self, file_path: str) -> List[Dict[str, Any]]:
        """
        ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
        
        Args:
            file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            List[Dict]: ì²˜ë¦¬ ë° ì²­í‚¹ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ì²­í‚¹ ê²°ê³¼ì— ë”°ë¼ ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìŒ)
        """
        filename = os.path.basename(file_path)
        logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘: {filename}")
        
        try:
            # 1. ë¬¸ì„œ ë¡œë“œ
            doc_data = self.document_loader.load_document(file_path)
            if not doc_data:
                logger.warning(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {filename}")
                return []
            
            # 2. í…ìŠ¤íŠ¸ ì •ê·œí™”
            normalized_content = self.text_normalizer.normalize(doc_data["content"])
            
            # 3. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self.metadata_extractor.extract_metadata(
                normalized_content, 
                filename, 
                doc_data["metadata"]
            )
            
            # í•´ì‹œ ê³„ì‚°
            content_hash = hashlib.md5(normalized_content.encode('utf-8')).hexdigest()
            metadata["file_hash"] = content_hash
            
            document = {
                "content": normalized_content,
                "metadata": metadata
            }
            
            # 4. ë¬¸ì„œ ë¶„í•  (ì²­í‚¹)
            # ê°•ì œë¡œ ì‘ì€ ì²­í¬ í¬ê¸°(500) ì‚¬ìš©í•˜ì—¬ í† í° í•œë„ë¥¼ ë„˜ì§€ ì•Šë„ë¡ í•¨
            # íŠ¹íˆ RTF, PDF íŒŒì¼ì˜ ê²½ìš° ë” ì‘ì€ ì²­í¬ ì‚¬ìš©
            file_ext = os.path.splitext(file_path)[1].lower()
            if file_ext in ['.rtf', '.pdf', '.docx']:
                forced_chunk_size = 400
                forced_overlap = 100
            else:
                forced_chunk_size = 800
                forced_overlap = 200
                
            chunked_docs = self.document_chunker.split_documents(
                [document], 
                forced_chunk_size,
                forced_overlap
            )
            
            logger.info(f"íŒŒì¼ '{filename}' ì²˜ë¦¬ ì™„ë£Œ: {len(chunked_docs)}ê°œ ì²­í¬ ìƒì„±")
            return chunked_docs
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {filename}, ì˜¤ë¥˜: {e}")
            logger.exception(e)
            return []
    
    def is_duplicate(self, content: str) -> bool:
        """
        ë‚´ìš© ì¤‘ë³µ ì—¬ë¶€ í™•ì¸
        
        Args:
            content: í™•ì¸í•  ë‚´ìš©
            
        Returns:
            bool: ì¤‘ë³µì´ë©´ True, ì•„ë‹ˆë©´ False
        """
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
        return content_hash in self.processed_hashes