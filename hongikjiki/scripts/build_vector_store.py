import logging
import os
import json
import hashlib
from datetime import datetime
from langchain.schema import Document
from hongikjiki.vector_store.chroma_store import ChromaVectorStore
from hongikjiki.vector_store.embeddings import get_embeddings

os.makedirs("logs", exist_ok=True)

logging.basicConfig(
    filename="logs/vector_store.log",
    filemode="a",
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

logger = logging.getLogger("VectorBuilder")

def hash_text(text):
    return hashlib.md5(text.encode("utf-8")).hexdigest()

def load_qa_pairs(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)

def convert_to_documents(qa_pairs):
    documents = []
    for pair in qa_pairs:
        try:
            question = pair["question"]
            answer = pair["answer"]
            tags = pair.get("tags", [])
            content = f"Q: {question}\nA: {answer}"
            base_string = question + ",".join(sorted(tags))
            source_id = "jungbub_qa_" + hash_text(base_string)

            metadata = {
                "source": source_id,
                "tags": tags
            }

            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°ê°€ ìˆë‹¤ë©´ ë³‘í•©
            if "metadata" in pair:
                metadata.update(pair["metadata"])

            documents.append(Document(
                page_content=content,
                metadata=metadata
            ))
        except KeyError as e:
            logger.warning(f"âš ï¸ QA í•„ë“œ ëˆ„ë½ìœ¼ë¡œ ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            continue
    return documents

def build_vector_store(qa_file, persist_dir="./data/vector_store", collection_name="hongikjiki_jungbub"):
    logger.info("ğŸ”¹ QA ë°ì´í„° ë¡œë“œ ì¤‘...")
    qa_pairs = load_qa_pairs(qa_file)
    logger.info(f"ğŸ”¹ ì´ {len(qa_pairs)}ê°œì˜ QA í•­ëª©")

    docs = convert_to_documents(qa_pairs)

    logger.info("ğŸ”¹ ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘...")
    vector_store = ChromaVectorStore(
        collection_name=collection_name,
        persist_directory=persist_dir,
        embeddings=get_embeddings("openai", model="text-embedding-3-small")
    )

    # ğŸ” ê¸°ì¡´ ë¬¸ì„œ source ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    existing_data = vector_store.get_all_documents()
    existing_metadatas = existing_data.get("metadatas", [])
    existing_sources = set(meta.get("source", "") for meta in existing_metadatas)

    # ğŸ†• ìƒˆë¡œìš´ ë¬¸ì„œë§Œ í•„í„°ë§
    new_docs = [doc for doc in docs if doc.metadata["source"] not in existing_sources]

    logger.info(f"ğŸ”¹ ì‹ ê·œ ì¶”ê°€í•  ë¬¸ì„œ ìˆ˜: {len(new_docs)}ê°œ")

    if new_docs:
        logger.info("ğŸ”¹ ë¬¸ì„œ ì„ë² ë”© ë° ì €ì¥ ì¤‘...")
        vector_ids = vector_store.add_texts(
            [doc.page_content for doc in new_docs],
            metadatas=[doc.metadata for doc in new_docs]
        ) or []

        if not vector_ids:
            logger.warning("â—ï¸ vector_idsê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì €ì¥ ì‹¤íŒ¨ ê°€ëŠ¥ì„± ìˆìŒ.")
    else:
        logger.info("âœ… ìƒˆë¡­ê²Œ ì¶”ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # print("ğŸ”¹ ë¬¸ì„œ ì„ë² ë”© ë° ì €ì¥ ì¤‘...")
    # vector_store.add_texts([doc.page_content for doc in docs], metadatas=[doc.metadata for doc in docs])

    # --- Extended metadata logging ---
    processed_log_path = "data/processed_files.json"
    os.makedirs(os.path.dirname(processed_log_path), exist_ok=True)

    # Load existing log if it exists
    if os.path.exists(processed_log_path):
        with open(processed_log_path, "r", encoding="utf-8") as f:
            processed_log = json.load(f)
    else:
        processed_log = {}

    timestamp = datetime.now().isoformat()

    if new_docs:
        for doc, vector_id in zip(new_docs, vector_ids):
            source_id = doc.metadata.get("source", "")
            hash_val = hash_text(doc.page_content)
            tags = doc.metadata.get("tags", [])
            processed_log[source_id] = {
                "hash": hash_val,
                "processed_time": timestamp,
                "chunks_count": 1,
                "tags": tags,
                "vector_ids": [vector_id],
                "source_text": doc.page_content[:100]  # Optional preview
            }

    # Save back to file
    with open(processed_log_path, "w", encoding="utf-8") as f:
        json.dump(processed_log, f, ensure_ascii=False, indent=2)

    vector_store.persist()
    logger.info("âœ… ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì™„ë£Œ!")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--qa_file", type=str, required=True, help="Path to QA JSON file")
    parser.add_argument("--persist_dir", type=str, default="./data/vector_store", help="Path to save Chroma vector store")
    parser.add_argument("--collection_name", type=str, default="hongikjiki_jungbub", help="Name of the Chroma collection")
    args = parser.parse_args()

    build_vector_store(args.qa_file, persist_dir=args.persist_dir, collection_name=args.collection_name)